{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 版权信息\n",
    "## 许可证\n",
    "\n",
    "本笔记本及其包含的所有内容（代码、文本、图片等），除非另有说明，均受MIT许可证保护：\n",
    "\n",
    "- **作者**：[Xianglong KONG]\n",
    "- **邮箱**：[kongxl@sdas.org]\n",
    "- **创建日期**：[2023-06-01]\n",
    "- **更新日期**：[2024-03-21]\n",
    "- **版本**：[v0.5]\n",
    "\n",
    "请在使用、分享或修改此笔记本时遵守上述许可证的条款。如果你对此笔记本有任何疑问或需要进一步的信息，请通过上面提供的邮箱与我联系。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "# import lux\n",
    "import numpy as np\n",
    "import pathogenprofiler as pp\n",
    "import json\n",
    "import tbprofiler as tp\n",
    "from collections import defaultdict\n",
    "import sys\n",
    "import argparse\n",
    "import re\n",
    "import csv\n",
    "import subprocess\n",
    "\n",
    "\n",
    "reverce={}\n",
    "reverce['A'] = 'T'\n",
    "reverce['T'] = 'A'\n",
    "reverce['G'] = 'C'\n",
    "reverce['C'] = 'G'\n",
    "\n",
    "aa_long2short = {\"Ala\":\"A\",\"Arg\":\"R\",\"Asn\":\"N\",\"Asp\":\"D\",\"Cys\":\"C\",\"Gln\":\"Q\",\"Glu\":\"E\",\"Gly\":\"G\",\"His\":\"H\",\"Ile\":\"I\",\"Leu\":\"L\",\"Lys\":\"K\",\"Met\":\"M\",\"Phe\":\"F\",\"Pro\":\"P\",\"Ser\":\"S\",\"Thr\":\"T\",\"Trp\":\"W\",\"Tyr\":\"Y\",\"Val\":\"V\",\"*\":\"*\", 'fs':'fs',\"?\":\"?\"}\n",
    "aa_short2long = {'A':'Ala', 'F':'Phe', 'C':'Cys', 'U':'Sec', 'D':'Asp', 'N':'Asn', 'E':'Glu', 'Q':'Gln', 'G':'Gly', 'H':'His', 'L':'Leu', 'I':'Ile', 'K':'Lys', 'O':'Pyl', 'M':'Met', 'P':'Pro', 'R':'Arg', 'S':'Ser', 'T':'Thr', 'V':'Val', 'W':'Trp', 'Y':'Tyr', '*':'STOP'}\n",
    "codon2aa = {\n",
    "    'ATA':'I', 'ATC':'I', 'ATT':'I', 'ATG':'M',\n",
    "    'ACA':'T', 'ACC':'T', 'ACG':'T', 'ACT':'T',\n",
    "    'AAC':'N', 'AAT':'N', 'AAA':'K', 'AAG':'K',\n",
    "    'AGC':'S', 'AGT':'S', 'AGA':'R', 'AGG':'R',\n",
    "    'CTA':'L', 'CTC':'L', 'CTG':'L', 'CTT':'L',\n",
    "    'CCA':'P', 'CCC':'P', 'CCG':'P', 'CCT':'P',\n",
    "    'CAC':'H', 'CAT':'H', 'CAA':'Q', 'CAG':'Q',\n",
    "    'CGA':'R', 'CGC':'R', 'CGG':'R', 'CGT':'R',\n",
    "    'GTA':'V', 'GTC':'V', 'GTG':'V', 'GTT':'V',\n",
    "    'GCA':'A', 'GCC':'A', 'GCG':'A', 'GCT':'A',\n",
    "    'GAC':'D', 'GAT':'D', 'GAA':'E', 'GAG':'E',\n",
    "    'GGA':'G', 'GGC':'G', 'GGG':'G', 'GGT':'G',\n",
    "    'TCA':'S', 'TCC':'S', 'TCG':'S', 'TCT':'S',\n",
    "    'TTC':'F', 'TTT':'F', 'TTA':'L', 'TTG':'L',\n",
    "    'TAC':'Y', 'TAT':'Y', 'TAA':'*', 'TAG':'*',\n",
    "    'TGC':'C', 'TGT':'C', 'TGA':'*', 'TGG':'W'}\n",
    "\n",
    "aa2codon = defaultdict(list)\n",
    "for codon in codon2aa:\n",
    "\taa2codon[codon2aa[codon]].append(codon)\n",
    "\n",
    "class geneRecord():\n",
    "\tstart = 0\n",
    "\tend = 0\n",
    "\tstrand = 1\n",
    "\tdescription = ''\n",
    "\tgid = ''\n",
    "\tdef __init__(self, gid, gname, start, end, strand, description):\n",
    "\t\tself.gid = gid\n",
    "\t\tself.gname = gname\n",
    "\t\tself.start = start\n",
    "\t\tself.end = end\n",
    "\t\tself.strand = strand\n",
    "\t\tself.description = description\n",
    "\n",
    "def processGffGenes(fileName):\n",
    "\tgenes = []\n",
    "\twith open(fileName) as fHandle:\n",
    "\t\tCrtype = 2\n",
    "\t\tCgstart = 3\n",
    "\t\tCgend = 4\n",
    "\t\tCgstrand = 6\n",
    "\t\tCgdescription = 8\n",
    "\t\tfor line in fHandle:\n",
    "\t\t\tif line.startswith('#'):\n",
    "\t\t\t\tcontinue\n",
    "\t\t\tif line.startswith('##'):\n",
    "\t\t\t\tcontinue\n",
    "\t\t\tarr = line.strip().split()\n",
    "\t\t\tif len(arr) and arr[2].strip() == 'gene':\n",
    "\t\t\t\tgid = \"\"\n",
    "\t\t\t\tgname = \"\"\n",
    "\t\t\t\tfor i in arr[Cgdescription].strip().split(';'):\n",
    "\t\t\t\t\tif \"ID=\" in i:\n",
    "\t\t\t\t\t\tgid = i.strip()[3:].replace(\"gene-Rv\", 'Rv')\n",
    "\t\t\t\t\tif \"Name=\" in i:\n",
    "\t\t\t\t\t\tgname = i.strip()[5:]\n",
    "\t\t\t\tif arr[Cgstrand].strip() == '+':\n",
    "\t\t\t\t\tstrand = 1\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tstrand = -1\n",
    "\t\t\t\tgenes.append(geneRecord(gid, gname, int(arr[Cgstart].strip()), int(arr[Cgend].strip()), strand, arr[Cgdescription]))\n",
    "\treturn genes\n",
    "\n",
    "def getGene(genes, geneId) -> geneRecord:\n",
    "\tfor gene in genes:\n",
    "\t\tif geneId in gene.gid:\n",
    "\t\t\treturn gene\n",
    "\treturn None\n",
    "\n",
    "def getGenebyName(genes, gname):\n",
    "\tfor gene in genes:\n",
    "\t\tif gname in gene.gname:\n",
    "\t\t\treturn gene\n",
    "\treturn None\n",
    "\n",
    "def get_nt_change(gene_object,ref_aa, pos, mut_aa):\n",
    "    ref_nt = ''\n",
    "    pos_aa_to_nt = []\n",
    "    pos_nt = 0\n",
    "    mut_nt = ''\n",
    "\n",
    "    g = gene_object\n",
    "    if g != None and g.strand > 0:\n",
    "        p = g.start + pos*3-3\n",
    "        pos_aa_to_nt =  [p,p+1,p+2]\n",
    "    elif g != None and g.strand < 0:\n",
    "        p = g.end - pos*3 + 1\n",
    "        pos_aa_to_nt = [p,p+1,p+2]       \n",
    "    return pos_aa_to_nt\n",
    "\n",
    "def get_nt_ref(ref_seq, gene_object, ref_aa, pos):\n",
    "\tref_nt = ''\n",
    "\tpos_aa_to_nt = []\n",
    "\tg = gene_object\n",
    "\tif g != None and g.strand > 0:\n",
    "\t\tp = g.start + pos*3-3\n",
    "\t\tpos_aa_to_nt =  [p,p+1,p+2]\n",
    "\t\tref_nt = ref_seq['AL123456.3'][p-1:p+2]\n",
    "\telif g != None and g.strand < 0:\n",
    "\t\tp = g.end - pos*3\n",
    "\t\tpos_aa_to_nt = [p,p+1,p+2]     \n",
    "\t\tref_nt = reverce[ref_seq['AL123456.3'][p+2]] + reverce[ref_seq['AL123456.3'][p+1]] + reverce[ref_seq['AL123456.3'][p]]\n",
    "\treturn ref_nt\n",
    "\n",
    "# Generate standardized SNP identifiers based on the position and VCF file.\n",
    "# snpEff result process \n",
    "def gff_load_cds(gff):\n",
    "    cds = []\n",
    "    for l in open(gff):\n",
    "        row = l.strip().split()\n",
    "        if len(row)<3:\n",
    "            continue\n",
    "        if row[2]!=\"CDS\":\n",
    "            continue\n",
    "        r = re.search(\"ID=(.+);+\",l)\n",
    "        id = r.group(1)\n",
    "        cds.append({\"chrom\":row[0],\"gene\":id,\"start\":int(row[3]),\"end\":int(row[4]),\"strand\":row[6]})\n",
    "    return cds\n",
    "\n",
    "def get_overlapping_gene(chrom,pos,genes):\n",
    "    genes = [g for g in genes if g[\"start\"]<=pos and g[\"end\"]>=pos and chrom==g[\"chrom\"]]\n",
    "    if len(genes)==0:\n",
    "        return None\n",
    "    else:\n",
    "        return genes[0]\n",
    "\n",
    "def get_codon_pos(chrom,pos,genes):\n",
    "    g = get_overlapping_gene(chrom,pos,genes)\n",
    "    if g==None:\n",
    "        return (None,None)\n",
    "    if g[\"strand\"]==\"+\":\n",
    "        codon_pos = (pos-g[\"start\"])//3 + 1\n",
    "    else:\n",
    "        codon_pos = (g[\"end\"] - pos )//3 + 1\n",
    "    return (g[\"gene\"],codon_pos)\n",
    "\n",
    "\n",
    "ref_file = '/raid/slyy/pipelines/ref/GCA_000195955.2_ASM19595v2/GCA_000195955.2_ASM19595v2_genomic.fna'\n",
    "gff_file = '/raid/slyy/pipelines/ref/GCA_000195955.2_ASM19595v2/GCA_000195955.2_ASM19595v2_genomic.gff'\n",
    "\n",
    "ref_seq = pp.fasta(ref_file).fa_dict\n",
    "gff = '/raid/slyy/pipelines/ref/GCA_000195955.2_ASM19595v2/GCA_000195955.2_ASM19595v2_genomic.gff'\n",
    "genes = processGffGenes(gff)\n",
    "\n",
    "ref = pp.fasta(ref_file).fa_dict\n",
    "cds = gff_load_cds(gff_file)\n",
    "coding = defaultdict(list)\n",
    "\n",
    "samples_list_dir = '/slyy-backup/international-tb/data_tables/sample_lists/'\n",
    "\n",
    "def write_list_to_file(l=[], fn = ''):\n",
    "    with open(f'{samples_list_dir}{fn}', 'w') as fp:\n",
    "        fp.write('\\n'.join(l))\n",
    "        \n",
    "def read_list_from_file(fn=''):\n",
    "    r = []\n",
    "    with open(f'{samples_list_dir}{fn}', 'r') as fp:\n",
    "        for line in fp:\n",
    "            r.append(line.strip())\n",
    "    return r\n",
    "\n",
    "international_samples = read_list_from_file('international_samples.txt')\n",
    "\n",
    "shandong_huada_samples = read_list_from_file('shandong_huada_1447.txt')\n",
    "\n",
    "ancient_samples = ['ERR651004','ERR651000','ERR979066','ERR046946','ERR979064','ERR979065','ERR651003','ERR979067','ERR979063','ERR979062','ERR651001','ERR650975','ERR651010']\n",
    "\n",
    "meta_file = '/slyy-backup/international-tb/data_tables/MTB_sra_international_samples_filtered_updated.csv'\n",
    "masked_consensus_dir = '/slyy-backup/international-tb/output/snippy-masked-vcfs/'\n",
    "output_dir_base = '/slyy-backup/international-tb/output/analysis/'\n",
    "meta_df = pd.read_csv(meta_file)\n",
    "\n",
    "shandong_sra_submission_file = '/slyy-backup/international-tb/data_tables/metadata-shandong-sra-submission.csv'\n",
    "shandong_sra_submission_df = pd.read_csv(shandong_sra_submission_file)\n",
    "\n",
    "# Read the data file using pandas.\n",
    "mdr_snp_cluster_file = '/slyy-backup/international-tb/data_tables/PZA_stats.csv'   \n",
    "mdr_snp_cluster = pd.read_csv(mdr_snp_cluster_file)\n",
    "\n",
    "mdr_snp_cluster = mdr_snp_cluster[mdr_snp_cluster['IID'].isin(set(mdr_snp_cluster['IID']) & set(international_samples))]\n",
    "mdr_snp_cluster = mdr_snp_cluster[mdr_snp_cluster['cluster2501'].notna()]\n",
    "mdr_snp_cluster_l2 = mdr_snp_cluster[mdr_snp_cluster['IID'].isin(meta_df[meta_df['main_lin'] == 'lineage2']['sample_name'])]\n",
    "mdr_snp_cluster_l4 = mdr_snp_cluster[mdr_snp_cluster['IID'].isin(meta_df[meta_df['main_lin'] == 'lineage4']['sample_name'])]\n",
    "\n",
    "meta_df_mdr = meta_df[meta_df['sample_name'].isin(mdr_snp_cluster['IID'].to_list())]\n",
    "meta_df_mdr = meta_df_mdr[meta_df_mdr['DR_type'].isin(['MDR-TB', 'Pre-XDR-TB'])]\n",
    "meta_df_mdr_l2 = meta_df_mdr[meta_df_mdr['main_lin'] == 'lineage2']\n",
    "meta_df_mdr_l4 = meta_df_mdr[meta_df_mdr['main_lin'] == 'lineage4']\n",
    "\n",
    "liym_L221_20240107 = read_list_from_file('liym-l2.2.1-20240107.txt')\n",
    "liym_L222_20240107 = read_list_from_file('liym-l2.2.2-20240107.txt')\n",
    "\n",
    "# File retrieval function.\n",
    "def findAllFile(base, ext):\n",
    "    for root, ds, fs in os.walk(base):\n",
    "        for f in fs:\n",
    "            if f.endswith(ext):\n",
    "                fullname = os.path.join(root, f)\n",
    "                filename = os.path.splitext(f)[0]\n",
    "                yield fullname, filename\n",
    "\n",
    "def findAllDir(rootdir):\n",
    "    for file in os.listdir(rootdir):\n",
    "        d = os.path.join(rootdir, file)\n",
    "        if os.path.isdir(d):\n",
    "            yield d, file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Shandong IDs to SRA IDs in the International dataset.\n",
    "\n",
    "dict_shandong_sra = dict(zip(shandong_sra_submission_df['sample_name'], shandong_sra_submission_df['accession']))\n",
    "# meta_df[meta_df.sample_name.isin(shandong_sra_submission_df['sample_name'])]['sample_name'] = meta_df[meta_df.sample_name.isin(shandong_sra_submission_df['sample_name'])]['sample_name'].map(dict_shandong_sra)\n",
    "meta_df.loc[meta_df.sample_name.isin(shandong_sra_submission_df['sample_name']), 'sample_name'] = meta_df[meta_df.sample_name.isin(shandong_sra_submission_df['sample_name'])]['sample_name'].map(dict_shandong_sra)\n",
    "meta_df.to_csv('/slyy-backup/international-tb/data_tables/MTB_sra_international_samples_filtered_updated.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the TB-Profiler processing script file.\n",
    "\n",
    "samples = set(meta_df.run_accession)\n",
    "\n",
    "output_file = '/slyy-backup/international-tb/output/tb-profiler-620/tb620-runme.sh'\n",
    "with open(output_file, 'w') as f:\n",
    "    for s in samples:\n",
    "        f.write(f'if [ ! -f results/{s}.results.json ]; then tb-profiler profile -1 /slyy-backup/international-tb/input-combine/{s}_1.fastq.gz -2 /slyy-backup/international-tb/input-combine/{s}_2.fastq.gz -p {s} --csv --call_whole_genome -t 10; fi;\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "# input: $ snp-dists -j 60 international.consensus.core2.fasta > international.vcf.consensus.core2.distance.tab\n",
    "\n",
    "# Cluster based on the distance matrix.\n",
    "# get clusters from distance matrix\n",
    "def find_cluster(t):\n",
    "    \n",
    "    # Change the row and column names to sample names.\n",
    "\n",
    "    samples = t.columns\n",
    "    t.index = samples\n",
    "\n",
    "    m = t\n",
    "    result = []\n",
    "    clusters = []\n",
    "    unclustered = samples\n",
    "    while(1):\n",
    "        unclustered = list(set(samples) - set(result))\n",
    "        s = unclustered\n",
    "        m = t.loc[s,s]\n",
    "        while (len(np.where(pd.isna(m))[0]) > 0):\n",
    "            na_row = np.where(pd.isna(m))[0][0]\n",
    "\n",
    "            indexes = m.loc[s[na_row],pd.notna(m.iloc[na_row])].index\n",
    "            s = list(set(indexes))\n",
    "            m = m.loc[s, s]\n",
    "        if len(s) == 0: break\n",
    "        if len(s) > 1: clusters.append(s)\n",
    "        result = result + s\n",
    "    return clusters\n",
    "\n",
    "# Cluster the results based on SNP distance threshold and save the results as CSV.\n",
    "\n",
    "def get_clusters(m, distance, result_dir,prefix='clusters-'): \n",
    "    md = m[m<=distance]\n",
    "    clusters = find_cluster(md)\n",
    "    with open(result_dir+prefix+str(distance)+'.csv', 'w', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        header = \"cluster\", \"sample\"\n",
    "        writer.writerow(header)\n",
    "        for c,s in zip(range(len(clusters)), clusters):\n",
    "            for i in range(len(s)):\n",
    "                writer.writerow([c] + [s[i]])\n",
    "            # t.loc[[s,s]].to_csv(result_dir+prefix+str(distance)+'.cluster'+str(c)+'.matrix'+'.csv')\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'findAllFile' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)\n",
      "\u001b[0;32m/tmp/ipykernel_2954261/430130603.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m      6\u001b[0m \u001b[0mall_coverage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfindAllFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbam_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'.coverage.out'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m      9\u001b[0m     \u001b[0ml\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sample_name'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\n",
      "\u001b[0;31mNameError\u001b[0m: name 'findAllFile' is not defined"
     ]
    }
   ],
   "source": [
    "# Summarize the output results of samtools coverage.\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "bam_dir = '/slyy-backup/international-tb/output/tb-profiler-430-2/bam'\n",
    "\n",
    "all_coverage = []\n",
    "for i,j in findAllFile(bam_dir, '.coverage.out'):\n",
    "    l = pd.read_csv(i, sep='\\t')\n",
    "    l['sample_name'] = j\n",
    "    all_coverage.append(l)\n",
    "\n",
    "df = pd.concat(all_coverage)\n",
    "df.to_csv('/slyy-backup/international-tb/output/tb-profiler-430-2/coverage-stats.csv', sep=',', encoding='utf_8_sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize the lineage information from TB Profiler results.\n",
    "import json\n",
    "\n",
    "result_dir = '/slyy-backup/international-tb/output/tb-combine/results'\n",
    "all_lnn = []\n",
    "for i,j in findAllFile(result_dir, '.json'):\n",
    "        with open(i) as f:\n",
    "            l = json.load(f)\n",
    "            all_lnn.append(pd.DataFrame({'sample_name':j, 'main_lin':l['main_lin'], 'sub_lin':l['sublin']}, index = ['sample_name']))\n",
    "df = pd.concat(all_lnn)\n",
    "\n",
    "df.to_csv('/slyy-backup/international-tb/output/tb-combine/lineage_stats.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize the drug resistance information from TB Profiler results.\n",
    "\n",
    "result_dir = '/slyy-backup/international-tb/output/tb-combine/results'\n",
    "all_dr = []\n",
    "for i,j in findAllFile(result_dir, '.json'):\n",
    "        with open(i) as f:\n",
    "            l = json.load(f)\n",
    "            # print(l['dr_variants'])\n",
    "            all_dr.append(pd.json_normalize(l, record_path='dr_variants'))\n",
    "\n",
    "df = pd.concat(all_dr)\n",
    "df.to_csv('/slyy-backup/international-tb/output/tb-combine/drug_res_stats.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio import SeqIO\n",
    "# Generate a merged sequence file using the recombined consensus sequences containing SNPs from the VCF file. Set the record ID as the sample name.\n",
    "\n",
    "consensus_file_dir = '/slyy-backup/international-tb/output/snippy-vcfs-core2/'\n",
    "ref_seq = '/raid/slyy/pipelines/ref/H37Rv.fasta'\n",
    "records = []\n",
    "for s in international_pza_dr_samples:\n",
    "    if s == 'Reference':\n",
    "        record = SeqIO.read(ref_seq, format='fasta')\n",
    "        record.id = 'Reference'\n",
    "        record.description = ''\n",
    "        records.append(record)\n",
    "    else:\n",
    "        record = SeqIO.read(consensus_file_dir + s + '.core2.masked.snps.vcf.gz.fa', format='fasta')\n",
    "        record.id = s\n",
    "        record.description = ''\n",
    "        records.append(record)\n",
    "        \n",
    "SeqIO.write(records, '/slyy-backup/international-tb/output/timetree/international.pza_1333.vcf.consensus.core2.fasta', format='fasta')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def generate_input_aln_file_masked_consensus(sample_list=[], out_file='consensus.masked.fasta', out_dir=output_dir_base, in_dir=masked_consensus_dir):\n",
    "    \n",
    "    \n",
    "    if not os.path.exists(out_dir):\n",
    "        os.mkdir(out_dir)\n",
    "    \n",
    "    records = []\n",
    "    # Generate a merged sequence file using the recombined consensus sequences that contain SNPs from the VCF file. Set the record ID as the sample name.\n",
    "    consensus_file_dir = masked_consensus_dir\n",
    "    ref_seq = '/raid/slyy/pipelines/ref/H37Rv.fasta'\n",
    "\n",
    "    record = SeqIO.read(ref_seq, format='fasta')\n",
    "    record.id = 'Reference'\n",
    "    records.append(record)\n",
    "\n",
    "    for s in sample_list:\n",
    "        record = SeqIO.read(in_dir + s + '.snps.subs.masked.vcf.gz.fa', format='fasta')\n",
    "        record.id = s\n",
    "        record.description = ''\n",
    "        records.append(record)\n",
    "\n",
    "    SeqIO.write(records, out_dir+out_file, format='fasta')\n",
    "    \n",
    "\n",
    "def iqtree_runner(seq_file='',outdir='',outfiles='',model='JC+I+G4', bs_reps='1000',alrt_reps='1000'):\n",
    "    ## the following routine runs iqtree looking for the best model and tree, with no bootstrap. \n",
    "    ## It writes results to text files, but also returns the best fitting model.\n",
    "    \n",
    "    a = subprocess.run(['iqtree','-s',seq_file,'-m',model,'-bb',bs_reps,'-alrt',alrt_reps,'-pre',outdir+outfiles])\n",
    "    print(a)\n",
    "    \n",
    "def treetime_runner(aln_file='', tree_file='', dates_file='', name_column='', date_column='', outdir='',clock_rate='4.684e-6'):\n",
    "    a = subprocess.run(['treetime', '--aln', aln_file, '--tree', tree_file, '--dates', dates_file, '--name-column', name_column, '--date-column', date_column, \n",
    "                        '--outdir', outdir,\n",
    "                        '--reroot', 'least-squares', '--reconstruct-tip-states', '--branch-length-mode', 'input', '--clock-filter', '3',\n",
    "                        '--relax', '1.0', '0.5', '--clock-rate', clock_rate])\n",
    "    print(a)\n",
    "    \n",
    "def iqtree_treetime_analysis(name='',samples=[]):\n",
    "    generate_input_aln_file_masked_consensus(sample_list=samples, \n",
    "                                             out_file=name+'.consensus.masked.fasta', \n",
    "                                             out_dir=output_dir_base+name+'/')\n",
    "    \n",
    "    iqtree_runner(seq_file=output_dir_base+name+'/'+name+'.consensus.masked.fasta',\n",
    "                  outdir=output_dir_base+name+'/',\n",
    "                  outfiles=name+'.consensus.masked',\n",
    "                  model='JC+I+G4', bs_reps='1000',alrt_reps='1000')\n",
    "    \n",
    "    treetime_runner(aln_file=output_dir_base+name+'/'+name+'.consensus.masked.fasta', \n",
    "                    tree_file=output_dir_base+name+'/'+name+'.consensus.masked.treefile', \n",
    "                    dates_file=meta_file, \n",
    "                    name_column='run_accession', \n",
    "                    date_column='collect_date', \n",
    "                    outdir=output_dir_base+name+'/',\n",
    "                    clock_rate='4.684e-6')\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
